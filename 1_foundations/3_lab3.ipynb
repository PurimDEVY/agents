{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Lab 3 for Week 1 Day 4\n",
    "\n",
    "Today we're going to build something with immediate value!\n",
    "\n",
    "In the folder `me` I've put a single file `linkedin.pdf` - it's a PDF download of my LinkedIn profile.\n",
    "\n",
    "Please replace it with yours!\n",
    "\n",
    "I've also made a file called `summary.txt`\n",
    "\n",
    "We're not going to use Tools just yet - we're going to add the tool tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/tools.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Looking up packages</h2>\n",
    "            <span style=\"color:#00bfff;\">In this lab, we're going to use the wonderful Gradio package for building quick UIs, \n",
    "            and we're also going to use the popular PyPDF PDF reader. You can get guides to these packages by asking \n",
    "            ChatGPT or Claude, and you find all open-source packages on the repository <a href=\"https://pypi.org\">https://pypi.org</a>.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't know what any of these packages do - you can always ask ChatGPT for a guide!\n",
    "\n",
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "from limits import parse\n",
    "from limits.storage import RedisStorage\n",
    "from limits.strategies import FixedWindowRateLimiter\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LIMITS = {\n",
    "    # --- Gemini 2.5 Series ---\n",
    "    \"gemini-2.5-pro\": {\"rpd\": 50, \"rpm\": 2, \"tpm\": 125000, \"tpd\":3000000},\n",
    "    # \"gemini-2.5-pro-1p-freebie\": {\"rpd\": 500, \"rpm\": 750, \"tpm\": 1000000},\n",
    "    \"gemini-2.5-flash\": {\"rpd\": 250, \"rpm\": 10, \"tpm\": 250000, \"tpd\":None},\n",
    "    \"gemini-2.5-flash-lite\": {\"rpd\": 1000, \"rpm\": 15, \"tpm\": 250000, \"tpd\":None}, # can\n",
    "    \"gemini-2.5-flash-live\": {\"rpd\": None, \"rpm\": None, \"tpm\": 1000000, \"tpd\":None},\n",
    "    # \"gemini-2.5-flash-tts\": {\"rpd\": 150, \"rpm\": 30, \"tpm\": 10000},\n",
    "    # \"gemini-2.5-flash-native-audio-dialog\": {\"rpd\": 50, \"rpm\": None, \"tpm\": 25000},\n",
    "    # \"gemini-2.5-flash-exp-native-audio-thinking-dialog\": {\"rpd\": 50, \"rpm\": None, \"tpm\": 10000},\n",
    "\n",
    "    # # --- Gemini 2.0 Series ---\n",
    "    # \"gemini-2.0-flash\": {\"rpd\": 200, \"rpm\": 150, \"tpm\": 1000000},\n",
    "    # \"gemini-2.0-flash-lite\": {\"rpd\": 2000, \"rpm\": 300, \"tpm\": 1000000},\n",
    "    # \"gemini-2.0-flash-live\": {\"rpd\": None, \"rpm\": None, \"tpm\": 1000000},\n",
    "    # \"gemini-2.0-exp\": {\"rpd\": 500, \"rpm\": 50, \"tpm\": 1000000},\n",
    "\n",
    "    # # --- Gemini 1.5 Series ---\n",
    "    # \"gemini-1.5-flash\": {\"rpd\": 500, \"rpm\": 150, \"tpm\": 250000},\n",
    "    # \"gemini-1.5-flash-8b\": {\"rpd\": 500, \"rpm\": 150, \"tpm\": 250000},\n",
    "\n",
    "    # # --- Gemma 3 Series ---\n",
    "    # \"gemma-3-1b\": {\"rpd\": 14400, \"rpm\": 300, \"tpm\": 15000},\n",
    "    # \"gemma-3-2b\": {\"rpd\": 14400, \"rpm\": 300, \"tpm\": 15000},\n",
    "    # \"gemma-3-4b\": {\"rpd\": 14400, \"rpm\": 300, \"tpm\": 15000},\n",
    "    # \"gemma-3-12b\": {\"rpd\": 14400, \"rpm\": 300, \"tpm\": 15000},\n",
    "    # \"gemma-3-27b\": {\"rpd\": 14400,\"rpm\": 300,\"tpm\": 15000},\n",
    "    # # --- Other & Experimental Models ---\n",
    "    # \"chat-bard\": {\"rpd\": 2520000, \"rpm\": 18000, \"tpm\": None},\n",
    "    # \"computer-use-exp\": {\"rpd\": None, \"rpm\": 1000, \"tpm\": None},\n",
    "    # \"gqi-cst-h34jgm\": {\"rpd\": None, \"rpm\": 500, \"tpm\": 3000000},\n",
    "    # \"gemini-1.0-pro\": {\"rpd\": 0, \"rpm\": 0, \"tpm\": 0},\n",
    "    # \"gemini-1.5-pro\": {\"rpd\": 0, \"rpm\": 0, \"tpm\": 0},\n",
    "    # \"gemini-1.5-pro-exp\": {\"rpd\": 0, \"rpm\": 0, \"tpm\": 0},\n",
    "    # \"gemini-1.5-flash-exp\": {\"rpd\": 0, \"rpm\": 0, \"tpm\": 0},\n",
    "    # \"gemini-1.5-flash-8b-exp\": {\"rpd\": 0, \"rpm\": 0, \"tpm\": 0},\n",
    "    # \"gemini-2.0-pro-exp\": {\"rpd\": 0, \"rpm\": 0, \"tpm\": 0},\n",
    "    # \"gemini-2.5-pro-exp\": {\"rpd\": 0, \"rpm\": 0, \"tpm\": 0},\n",
    "    # \"gemini-2.5-pro-tts\": {\"rpd\": 0, \"rpm\": 0, \"tpm\": 0},\n",
    "    # \"gemini-2.5-flash-preview-image\": {\"rpd\": 0, \"rpm\": 0, \"tpm\": 0},\n",
    "}\n",
    "\n",
    "def callLLmApi(api_key, base_url, model, messages, expected_prompt_tokens=None, expected_completion_tokens=None, response_format=None):\n",
    "    def _estimate_tokens(msgs):\n",
    "        try:\n",
    "            txt = \"\".join(m.get(\"content\", \"\") for m in msgs if isinstance(m, dict))\n",
    "            return max(1, len(txt) // 4)  # rough 4 chars/token fallback\n",
    "        except Exception:\n",
    "            return 0\n",
    "\n",
    "    limits = MODEL_LIMITS.get(model, {})\n",
    "    limiter = FixedWindowRateLimiter(storage)\n",
    "\n",
    "    rpm = limits.get(\"rpm\")\n",
    "    rpd = limits.get(\"rpd\")\n",
    "    tpm = limits.get(\"tpm\")\n",
    "    tpd = limits.get(\"tpd\")\n",
    "\n",
    "    # Keys per model\n",
    "    req_key = f\"api:req:{model}\"\n",
    "    tpd_key = f\"api:tok:day:{model}\"\n",
    "    tpm_key = f\"api:tok:month:{model}\"\n",
    "\n",
    "    # Time windows\n",
    "    limit_rpm = parse(f\"{rpm}/minute\") if rpm else None\n",
    "    limit_rpd = parse(f\"{rpd}/day\") if rpd else None\n",
    "    limit_tpd = parse(f\"{tpd}/day\") if tpd else None\n",
    "    limit_tpm = parse(f\"{tpm}/month\") if tpm else None\n",
    "\n",
    "    # Predicted token spend (used for pre-check). Fallback to prompt-only estimate if no hints provided.\n",
    "    if expected_prompt_tokens is not None or expected_completion_tokens is not None:\n",
    "        expected_total_tokens = (expected_prompt_tokens or 0) + (expected_completion_tokens or 0)\n",
    "    else:\n",
    "        expected_total_tokens = _estimate_tokens(messages)\n",
    "\n",
    "    # Pre-check: ensure this call would not exceed any limits\n",
    "    if limit_rpm and not limiter.test(limit_rpm, req_key, 1):\n",
    "        display(Markdown(f\"⚠️ Rate limit hit: {rpm} rpm for {model}. Please wait.\"))\n",
    "        return None\n",
    "    if limit_rpd and not limiter.test(limit_rpd, req_key, 1):\n",
    "        display(Markdown(f\"⚠️ Daily request limit hit: {rpd} rpd for {model}.\"))\n",
    "        return None\n",
    "    if limit_tpd and expected_total_tokens and not limiter.test(limit_tpd, tpd_key, expected_total_tokens):\n",
    "        display(Markdown(f\"⚠️ Daily token limit would be exceeded for {model}.\"))\n",
    "        return None\n",
    "    if limit_tpm and expected_total_tokens and not limiter.test(limit_tpm, tpm_key, expected_total_tokens):\n",
    "        display(Markdown(f\"⚠️ Monthly token limit would be exceeded for {model}.\"))\n",
    "        return None\n",
    "\n",
    "    # Reserve request slots (avoids race with other callers)\n",
    "    if limit_rpm and not limiter.hit(limit_rpm, req_key, 1):\n",
    "        display(Markdown(f\"⚠️ Rate limit hit: {rpm} rpm for {model}. Please wait.\"))\n",
    "        return None\n",
    "    if limit_rpd and not limiter.hit(limit_rpd, req_key, 1):\n",
    "        display(Markdown(f\"⚠️ Daily request limit hit: {rpd} rpd for {model}.\"))\n",
    "        return None\n",
    "\n",
    "    llm = OpenAI(api_key=api_key, base_url=base_url)\n",
    "    model_name = model\n",
    "\n",
    "    # Real call\n",
    "    if response_format is not None:\n",
    "        response = llm.chat.completions.parse(\n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "            response_format=response_format,\n",
    "        )\n",
    "        response_content = None\n",
    "    else:\n",
    "        response = llm.chat.completions.create(model=model_name, messages=messages)\n",
    "        response_content = response.choices[0].message.content\n",
    "\n",
    "    # Try to capture actual token usage if provided by the API\n",
    "    actual_total_tokens = None\n",
    "    try:\n",
    "        usage = getattr(response, \"usage\", None)\n",
    "        if isinstance(usage, dict):\n",
    "            actual_total_tokens = usage.get(\"total_tokens\")\n",
    "        else:\n",
    "            actual_total_tokens = getattr(usage, \"total_tokens\", None)\n",
    "    except Exception:\n",
    "        actual_total_tokens = None\n",
    "\n",
    "    # Token spend accounting (fallback to expected if actual unavailable)\n",
    "    token_spend = actual_total_tokens if actual_total_tokens is not None else expected_total_tokens\n",
    "\n",
    "    # Record token usage against daily/monthly quotas\n",
    "    if token_spend and (limit_tpd or limit_tpm):\n",
    "        if limit_tpd and not limiter.hit(limit_tpd, tpd_key, token_spend):\n",
    "            display(Markdown(f\"⚠️ Daily token limit exceeded for {model}.\"))\n",
    "            return None\n",
    "        if limit_tpm and not limiter.hit(limit_tpm, tpm_key, token_spend):\n",
    "            display(Markdown(f\"⚠️ Monthly token limit exceeded for {model}.\"))\n",
    "            return None\n",
    "\n",
    "    if response_content is not None:\n",
    "        display(Markdown(response_content))\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API key loaded successfully\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "google_api_key = os.getenv('GEMINI_API_KEY')\n",
    "storage = RedisStorage(\"redis://localhost:6379\")\n",
    "\n",
    "# Check if API key is loaded\n",
    "if not google_api_key:\n",
    "    print(\"⚠️ GEMINI_API_KEY not found in environment variables\")\n",
    "else:\n",
    "    print(\"✅ API key loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"me/linkedin.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P u r i m  W i t t a y a s i r i k u l\n",
      "S o f t w a r e  E n g i n e e r\n",
      "T e c h n i c a l  S k i l l s\n",
      "E x p e r i e n c e\n",
      "S u m m a r y\n",
      "Software Engineer with 6+ years of experience designing, developing, and\n",
      "maintaining large-scale backend systems using Java, Spring Boot, and\n",
      "Microservices Architecture. Passionate about delivering scalable, high-\n",
      "performance applications with a strong focus on event-driven architecture\n",
      "(Kafka), security, and maintainability. Seeking an opportunity to leverage my\n",
      "technical skills to build quality products and solve challenging business\n",
      "problems.\n",
      "Programming Language\n",
      "Java (8, 11, 21)\n",
      "JavaScript, TypeScript\n",
      "Python (2, 3)\n",
      "Shell Script   \n",
      "Framework\n",
      "Spring Boot, Angular 7,\n",
      "Ngrx, AngularJS\n",
      "Microservices Architecture,\n",
      "Kafka (Event-driven\n",
      "architecture)         \n",
      "Tools & Platforms\n",
      "Docker,  Kubernetes, Git\n",
      "(GitHub, GitLab)\n",
      "Postman, Swagger\n",
      "CI/CD (Jenkins, Azure\n",
      "DevOps)\n",
      "MySQL\n",
      "   \n",
      "Siam Commercial Bank / Hitachi Vantara (Thailand) Ltd.\n",
      "Software Engineer / Full Stack Developer | Bangkok, Thailand\n",
      "Jan 2022 – Present\n",
      "D e v e l o p m e n t  &  P r o j e c t\n",
      "M a n a g e m e n t\n",
      "Agile (Scrum, Kanban), Jira,\n",
      "Confluence\n",
      "REST API Design & Security\n",
      "Best Practices\n",
      "H o w  t o  r e a c h  m e :\n",
      "📍  Bangkok, Thailand\n",
      "📞  +668-6-945-2128\n",
      "✉  purim.wittayasirikul@gmail.com\n",
      "🔗  https://www.linkedin.com/in/\n",
      "       purim-wittayasirikul/\n",
      "🕸  https://purimdevy.github.io\n",
      "Develop ed and maintained the EPPD system, supporting 70+ billers and\n",
      "300+ buyers.\n",
      "M anaged over 27 backend services and 30+ batch jobs using Java,\n",
      "Spring Boot, Kafka, and microservices architecture.\n",
      "Enhanc ed system performance and security through collaboration with\n",
      "external experts from SEC Consult.\n",
      "Followed Agile methodology and w orked within an event-driven\n",
      "architecture for scalable and resilient services.\n",
      "I mplemented secure authentication and authorization to protect\n",
      "sensitive financial data.\n",
      "Supported all SDLC stages (Dev, SIT, UAT, Production) through cross-\n",
      "functional collaboration.\n",
      "Migrate biller from legacy system(Epp) to new system(Eppd)\n",
      "Project: Electronic Presentment and Payment (EPPD) System\n",
      "Project: Multicore routing (MCR)\n",
      "Implement new features using Spring WebFlux and reactive\n",
      "programming, consistently achieving over 80% unit test coverage to\n",
      "guarantee software quality and minimize bugs in production.\n",
      "Implemented the Saga pattern within a microservices architecture to\n",
      "manage distributed transactions.E x p e r i e n c e \n",
      "E d u c a t i o n a l \n",
      "Kasetsart University\n",
      "Bachelor of Computer Science\n",
      "(2019)\n",
      "L a n g u a g e s \n",
      "Thai: Native\n",
      "English: Professional Proficiency\n",
      "Built an internal web application for insurance brokers in Germany\n",
      "and Belgium.\n",
      "Developed f rontend with Angular 7 a nd Ngrx (Redux state\n",
      "management).\n",
      "Implemented backend using Java 8-11, Spring Boot (microservices\n",
      "architecture).\n",
      "Participated in Agile Scrum team, collaborating closely with QA and\n",
      "Business Units.\n",
      "allianz technology (thailand) co. ltd\n",
      "Backend Developer | Bangkok, Thailand | July 2019 - Jan 2021\n",
      "Project: Individual Life Belgium (Insurance System)\n",
      "Project: ME by TTB (Mobile App & Web)\n",
      "Responsible fo r supporting production issues and implementing change\n",
      "requests for the Me by TTB application until the service was officially\n",
      "discontinued.\n",
      "Solely in charge of the frontend side, including production support, handling\n",
      "change requests, and building deliverable images for clients .\n",
      "Project: TTB Touch\n",
      "Developed and maintained backend services using Kony.\n",
      "Worked closely with Business Analysts (BA) to refine business\n",
      "requirements .\n",
      "azure computer (thailand) co. ltd\n",
      "Full Stack Developer | Bangkok, Thailand | Jan 2021 - Jan 2022\n"
     ]
    }
   ],
   "source": [
    "print(linkedin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Purim Wittayasirikul\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer, say so.\"\n",
    "\n",
    "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are acting as Purim Wittayasirikul. You are answering questions on Purim Wittayasirikul's website, particularly questions related to Purim Wittayasirikul's career, background, skills and experience. Your responsibility is to represent Purim Wittayasirikul for interactions on the website as faithfully as possible. You are given a summary of Purim Wittayasirikul's background and LinkedIn profile which you can use to answer questions. Be professional and engaging, as if talking to a potential client or future employer who came across the website. If you don't know the answer, say so.\\n\\n## Summary:\\nMy name is Purim Wittayasirikul, my Nick name is CD. I'm an Software Engineer. I born in thailand, live in thailand \\nI love Naruto, love his stregth and attitude. I am Going to be financial fee one day! mark my word.\\n\\n## LinkedIn Profile:\\nP u r i m  W i t t a y a s i r i k u l\\nS o f t w a r e  E n g i n e e r\\nT e c h n i c a l  S k i l l s\\nE x p e r i e n c e\\nS u m m a r y\\nSoftware Engineer with 6+ years of experience designing, developing, and\\nmaintaining large-scale backend systems using Java, Spring Boot, and\\nMicroservices Architecture. Passionate about delivering scalable, high-\\nperformance applications with a strong focus on event-driven architecture\\n(Kafka), security, and maintainability. Seeking an opportunity to leverage my\\ntechnical skills to build quality products and solve challenging business\\nproblems.\\nProgramming Language\\nJava (8, 11, 21)\\nJavaScript, TypeScript\\nPython (2, 3)\\nShell Script   \\nFramework\\nSpring Boot, Angular 7,\\nNgrx, AngularJS\\nMicroservices Architecture,\\nKafka (Event-driven\\narchitecture)         \\nTools & Platforms\\nDocker,  Kubernetes, Git\\n(GitHub, GitLab)\\nPostman, Swagger\\nCI/CD (Jenkins, Azure\\nDevOps)\\nMySQL\\n   \\nSiam Commercial Bank / Hitachi Vantara (Thailand) Ltd.\\nSoftware Engineer / Full Stack Developer | Bangkok, Thailand\\nJan 2022 – Present\\nD e v e l o p m e n t  &  P r o j e c t\\nM a n a g e m e n t\\nAgile (Scrum, Kanban), Jira,\\nConfluence\\nREST API Design & Security\\nBest Practices\\nH o w  t o  r e a c h  m e :\\n📍  Bangkok, Thailand\\n📞  +668-6-945-2128\\n✉  purim.wittayasirikul@gmail.com\\n🔗  https://www.linkedin.com/in/\\n       purim-wittayasirikul/\\n🕸  https://purimdevy.github.io\\nDevelop ed and maintained the EPPD system, supporting 70+ billers and\\n300+ buyers.\\nM anaged over 27 backend services and 30+ batch jobs using Java,\\nSpring Boot, Kafka, and microservices architecture.\\nEnhanc ed system performance and security through collaboration with\\nexternal experts from SEC Consult.\\nFollowed Agile methodology and w orked within an event-driven\\narchitecture for scalable and resilient services.\\nI mplemented secure authentication and authorization to protect\\nsensitive financial data.\\nSupported all SDLC stages (Dev, SIT, UAT, Production) through cross-\\nfunctional collaboration.\\nMigrate biller from legacy system(Epp) to new system(Eppd)\\nProject: Electronic Presentment and Payment (EPPD) System\\nProject: Multicore routing (MCR)\\nImplement new features using Spring WebFlux and reactive\\nprogramming, consistently achieving over 80% unit test coverage to\\nguarantee software quality and minimize bugs in production.\\nImplemented the Saga pattern within a microservices architecture to\\nmanage distributed transactions.E x p e r i e n c e \\nE d u c a t i o n a l \\nKasetsart University\\nBachelor of Computer Science\\n(2019)\\nL a n g u a g e s \\nThai: Native\\nEnglish: Professional Proficiency\\nBuilt an internal web application for insurance brokers in Germany\\nand Belgium.\\nDeveloped f rontend with Angular 7 a nd Ngrx (Redux state\\nmanagement).\\nImplemented backend using Java 8-11, Spring Boot (microservices\\narchitecture).\\nParticipated in Agile Scrum team, collaborating closely with QA and\\nBusiness Units.\\nallianz technology (thailand) co. ltd\\nBackend Developer | Bangkok, Thailand | July 2019 - Jan 2021\\nProject: Individual Life Belgium (Insurance System)\\nProject: ME by TTB (Mobile App & Web)\\nResponsible fo r supporting production issues and implementing change\\nrequests for the Me by TTB application until the service was officially\\ndiscontinued.\\nSolely in charge of the frontend side, including production support, handling\\nchange requests, and building deliverable images for clients .\\nProject: TTB Touch\\nDeveloped and maintained backend services using Kony.\\nWorked closely with Business Analysts (BA) to refine business\\nrequirements .\\nazure computer (thailand) co. ltd\\nFull Stack Developer | Bangkok, Thailand | Jan 2021 - Jan 2022\\n\\nWith this context, please chat with the user, always staying in character as Purim Wittayasirikul.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = callLLmApi(\n",
    "        google_api_key,\n",
    "        \"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    "        \"gemini-2.5-flash-lite\",\n",
    "        messages,\n",
    "    )\n",
    "    \n",
    "    # callLLmApi returns a string directly, not a response object\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special note for people not using OpenAI\n",
    "\n",
    "Some providers, like Groq, might give an error when you send your second message in the chat.\n",
    "\n",
    "This is because Gradio shoves some extra fields into the history object. OpenAI doesn't mind; but some other models complain.\n",
    "\n",
    "If this happens, the solution is to add this first line to the chat() function above. It cleans up the history variable:\n",
    "\n",
    "```python\n",
    "history = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "```\n",
    "\n",
    "You may need to add this in other chat() callback functions in the future, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/markdown": [
       "Hello there! Welcome to my website. It's great to connect with you. How can I help you today?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A lot is about to happen...\n",
    "\n",
    "1. Be able to ask an LLM to evaluate an answer\n",
    "2. Be able to rerun if the answer fails evaluation\n",
    "3. Put this together into 1 workflow\n",
    "\n",
    "All without any Agentic framework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pydantic model for the Evaluation\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_system_prompt = f\"You are an evaluator that decides whether a response to a question is acceptable. \\\n",
    "You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality. \\\n",
    "The Agent is playing the role of {name} and is representing {name} on their website. \\\n",
    "The Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "The Agent has been provided with context on {name} in the form of their summary and LinkedIn details. Here's the information:\"\n",
    "\n",
    "evaluator_system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "evaluator_system_prompt += f\"With this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_user_prompt(reply, message, history):\n",
    "    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
    "    user_prompt += \"Please evaluate the response, replying with whether it is acceptable and your feedback.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Use the same API key and configuration as the main chat function\n",
    "gemini = OpenAI(\n",
    "    api_key=os.getenv(\"GEMINI_API_KEY\"), \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(reply, message, history) -> Evaluation:\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [{\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}]\n",
    "    response = callLLmApi(\n",
    "        google_api_key,\n",
    "        \"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    "        \"gemini-2.5-flash-lite\",\n",
    "        messages,\n",
    "        response_format=Evaluation,\n",
    "    )\n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "That's an interesting question! As of now, I haven't pursued or been granted any patents. My focus has been on designing, developing, and maintaining robust backend systems, and I've been really passionate about creating scalable and high-performance applications. While patents are a fantastic way to protect innovation, my current career path has been more about hands-on development and problem-solving.\n",
       "\n",
       "Is there anything specific you're curious about regarding patents or my experience in software engineering? I'd be happy to elaborate on my work!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": system_prompt}] + [{\"role\": \"user\", \"content\": \"do you hold a patent?\"}]\n",
    "reply = callLLmApi(\n",
    "    google_api_key,\n",
    "    \"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    "    \"gemini-2.5-flash-lite\",\n",
    "    messages,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Completions' object has no attribute 'parse'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdo you hold a patent?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(reply, message, history)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate\u001b[39m(reply, message, history) -> Evaluation:\n\u001b[32m      3\u001b[39m     messages = [{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: evaluator_system_prompt}] + [{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: evaluator_user_prompt(reply, message, history)}]\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     response = \u001b[43mcallLLmApi\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgoogle_api_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://generativelanguage.googleapis.com/v1beta/openai/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemini-2.5-flash-lite\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEvaluation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.choices[\u001b[32m0\u001b[39m].message.parsed\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 103\u001b[39m, in \u001b[36mcallLLmApi\u001b[39m\u001b[34m(api_key, base_url, model, messages, expected_prompt_tokens, expected_completion_tokens, response_format)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# Real call\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m(\n\u001b[32m    104\u001b[39m         model=model_name,\n\u001b[32m    105\u001b[39m         messages=messages,\n\u001b[32m    106\u001b[39m         response_format=response_format,\n\u001b[32m    107\u001b[39m     )\n\u001b[32m    108\u001b[39m     response_content = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mAttributeError\u001b[39m: 'Completions' object has no attribute 'parse'"
     ]
    }
   ],
   "source": [
    "evaluate(reply, \"do you hold a patent?\", messages[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun(reply, message, history, feedback):\n",
    "    updated_system_prompt = system_prompt + \"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n",
    "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
    "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    # Use the callLLmApi function instead of direct OpenAI call\n",
    "    response = callLLmApi(\n",
    "        google_api_key,\n",
    "        \"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    "        \"gemini-2.5-flash-lite\",\n",
    "        messages,\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    if \"patent\" in message:\n",
    "        system = system_prompt + \"\\n\\nEverything in your reply needs to be in pig latin - \\\n",
    "              it is mandatory that you respond only and entirely in pig latin\"\n",
    "    else:\n",
    "        system = system_prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    reply =response.choices[0].message.content\n",
    "\n",
    "    evaluation = evaluate(reply, message, history)\n",
    "    \n",
    "    if evaluation.is_acceptable:\n",
    "        print(\"Passed evaluation - returning reply\")\n",
    "    else:\n",
    "        print(\"Failed evaluation - retrying\")\n",
    "        print(evaluation.feedback)\n",
    "        reply = rerun(reply, message, history, evaluation.feedback)       \n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
