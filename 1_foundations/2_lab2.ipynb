{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the Second Lab - Week 1, Day 3\n",
    "\n",
    "Today we will work with lots of models! This is a way to get comfortable with APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Important point - please read</h2>\n",
    "            <span style=\"color:#ff7800;\">The way I collaborate with you may be different to other courses you've taken. I prefer not to type code while you watch. Rather, I execute Jupyter Labs, like this, and give you an intuition for what's going on. My suggestion is that you carefully execute this yourself, <b>after</b> watching the lecture. Add print statements to understand what's going on, and then come up with your own variations.<br/><br/>If you have time, I'd love it if you submit a PR for changes in the community_contributions folder - instructions in the resources. Also, if you have a Github account, use this to showcase your variations. Not only is this essential practice, but it demonstrates your skills to others, including perhaps future clients or employers...\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with imports - ask ChatGPT to explain any package that you don't know\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display\n",
    "from limits import parse\n",
    "from limits.storage import RedisStorage\n",
    "from limits.strategies import FixedWindowRateLimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always remember to do this!\n",
    "load_dotenv(override=True)\n",
    "storage = RedisStorage(\"redis://localhost:6379\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LIMITS = {\n",
    "    # --- Gemini 1.5 Series ---\n",
    "    \"gemini-1.5-flash\": {\n",
    "        \"rpd\": 50,\n",
    "        \"rpm\": 15,\n",
    "        \"tpm\": 250000\n",
    "    },\n",
    "    \"gemini-1.5-flash-8b\": {\n",
    "        \"rpd\": 50,\n",
    "        \"rpm\": 15,\n",
    "        \"tpm\": 250000\n",
    "    },\n",
    "    \"gemini-1.5-flash-exp\": {\n",
    "        \"rpd\": 0,\n",
    "        \"rpm\": 0,\n",
    "        \"tpm\": 0\n",
    "    },\n",
    "    \"gemini-1.5-flash-8b-exp\": {\n",
    "        \"rpd\": 0,\n",
    "        \"rpm\": 0,\n",
    "        \"tpm\": 0\n",
    "    },\n",
    "\n",
    "    # --- Gemini 2.0 Series ---\n",
    "    \"gemini-2.0-flash\": {\n",
    "        \"rpd\": 200,\n",
    "        \"rpm\": 15,\n",
    "        \"tpm\": 1000000\n",
    "    },\n",
    "    \"gemini-2.0-flash-lite\": {\n",
    "        \"rpd\": 200,\n",
    "        \"rpm\": 30,\n",
    "        \"tpm\": 1000000\n",
    "    },\n",
    "    \"gemini-2.0-flash-exp\": {\n",
    "        \"rpd\": 50,\n",
    "        \"rpm\": 10,\n",
    "        \"tpm\": 250000\n",
    "    },\n",
    "    \"gemini-2.0-flash-exp-audio\": {\n",
    "        # Note: RPD and RPM for this model were not in your list, assuming reasonable values.\n",
    "        \"rpd\": 50, # Placeholder\n",
    "        \"rpm\": 2,  # Placeholder\n",
    "        \"tpm\": 4000000\n",
    "    },\n",
    "    \"gemini-2.0-flash-exp-image\": {\n",
    "        # Note: RPD and RPM for this model were not in your list, assuming reasonable values.\n",
    "        \"rpd\": 50, # Placeholder\n",
    "        \"rpm\": 2,  # Placeholder\n",
    "        \"tpm\": 4000000\n",
    "    },\n",
    "    \"gemini-2.0-flash-live\": {\n",
    "        \"rpd\": None, # Unlimited\n",
    "        \"rpm\": None, # Unlimited\n",
    "        \"tpm\": 1000000\n",
    "    },\n",
    "    \"gemini-2.0-flash-preview-image-generation\": {\n",
    "        \"rpd\": 100,\n",
    "        \"rpm\": 10,\n",
    "        \"tpm\": 200000\n",
    "    },\n",
    "\n",
    "    # --- Gemini 2.5 Series ---\n",
    "    \"gemini-2.5-flash\": {\n",
    "        \"rpd\": 250,\n",
    "        \"rpm\": 10,\n",
    "        \"tpm\": 250000\n",
    "    },\n",
    "    \"gemini-2.5-flash-lite\": {\n",
    "        \"rpd\": 1000,\n",
    "        \"rpm\": 15,\n",
    "        \"tpm\": 250000\n",
    "    },\n",
    "    \"gemini-2.5-flash-exp-native-audio-thinking-dialog\": {\n",
    "        \"rpd\": 5,\n",
    "        \"rpm\": None, # Unlimited\n",
    "        \"tpm\": 10000\n",
    "    },\n",
    "    \"gemini-2.5-flash-native-audio-dialog\": {\n",
    "        \"rpd\": 5,\n",
    "        \"rpm\": None, # Unlimited\n",
    "        \"tpm\": 25000\n",
    "    },\n",
    "    \"gemini-2.5-flash-live\": {\n",
    "        \"rpd\": None, # Unlimited\n",
    "        \"rpm\": None, # Unlimited\n",
    "        \"tpm\": None  # Assuming unlimited if not specified\n",
    "    },\n",
    "    \"gemini-2.5-flash-preview-image\": {\n",
    "        \"rpd\": 50,\n",
    "        \"rpm\": 20,\n",
    "        \"tpm\": 200000\n",
    "    },\n",
    "    \"gemini-2.5-flash-tts\": {\n",
    "        \"rpd\": 15,\n",
    "        \"rpm\": 3,\n",
    "        \"tpm\": 10000\n",
    "    },\n",
    "\n",
    "    # --- Other Models ---\n",
    "    \"learnlm-2.0-flash-experimental\": {\n",
    "        \"rpd\": 1500,\n",
    "        \"rpm\": 15,\n",
    "        \"tpm\": None # TPM not specified in your list\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google API Key exists and begins AI\n"
     ]
    }
   ],
   "source": [
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "# openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "# anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GEMINI_API_KEY')\n",
    "# deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "# groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "# if openai_api_key:\n",
    "#     print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "# else:\n",
    "#     print(\"OpenAI API Key not set\")\n",
    "    \n",
    "# if anthropic_api_key:\n",
    "#     print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "# else:\n",
    "#     print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "# if deepseek_api_key:\n",
    "#     print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "# else:\n",
    "#     print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "# if groq_api_key:\n",
    "#     print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "# else:\n",
    "#     print(\"Groq API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def callLLmApi(api_key,base_url,model):\n",
    "#     llm = OpenAI(api_key=api_key, base_url=base_url)\n",
    "#     model_name = model\n",
    "\n",
    "#     response = llm.chat.completions.create(model=model_name, messages=messages)\n",
    "#     response_content = response.choices[0].message.content\n",
    "\n",
    "#     display(Markdown(response_content))\n",
    "#     return response_content\n",
    "# Updated callLLmApi function with rate limiting\n",
    "def callLLmApi(api_key, base_url, model, messages):\n",
    "    # Initialize rate limiter for this model\n",
    "    if model in MODEL_LIMITS:\n",
    "        limits = MODEL_LIMITS[model]\n",
    "        # Create rate limiter for requests per minute (rpm)\n",
    "        if limits.get(\"rpm\"):\n",
    "            rpm_limiter = FixedWindowRateLimiter(storage)\n",
    "            rpm_limit = parse(f\"{limits['rpm']}/minute\")\n",
    "        else:\n",
    "            rpm_limiter = None\n",
    "            rpm_limit = None\n",
    "    else:\n",
    "        rpm_limiter = None\n",
    "        rpm_limit = None\n",
    "    \n",
    "    llm = OpenAI(api_key=api_key, base_url=base_url)\n",
    "    model_name = model\n",
    "\n",
    "    response = llm.chat.completions.create(model=model_name, messages=messages)\n",
    "    response_content = response.choices[0].message.content\n",
    "\n",
    "    display(Markdown(response_content))\n",
    "    return response_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"Answer only with the question, no explanation.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the updated callLLmApi function with rate limiting\n",
    "# Example usage with a model that has rate limits\n",
    "\n",
    "# Test with a model from MODEL_LIMITS\n",
    "test_model = \"gemini-2.0-flash\"\n",
    "test_messages = [{\"role\": \"user\", \"content\": \"Hello, this is a test message.\"}]\n",
    "\n",
    "print(f\"Testing rate limiting for model: {test_model}\")\n",
    "print(f\"Rate limits for this model: {MODEL_LIMITS.get(test_model, 'No limits defined')}\")\n",
    "\n",
    "# Note: This will only work if you have the appropriate API keys set up\n",
    "# Uncomment the line below to test (make sure you have GEMINI_API_KEY set)\n",
    "# result = callLLmApi(google_api_key, \"https://generativelanguage.googleapis.com/v1beta/openai/\", test_model, test_messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing rate limiting for model: gemini-2.0-flash\n",
      "üìä Rate limits: {'rpd': 200, 'rpm': 15, 'tpm': 1000000}\n",
      "\n",
      "Attempt 1:\n",
      "  ‚úÖ Rate limit check passed - would make API call\n",
      "Attempt 2:\n",
      "  ‚úÖ Rate limit check passed - would make API call\n",
      "Attempt 3:\n",
      "  ‚úÖ Rate limit check passed - would make API call\n",
      "Attempt 4:\n",
      "  ‚úÖ Rate limit check passed - would make API call\n",
      "Attempt 5:\n",
      "  ‚úÖ Rate limit check passed - would make API call\n",
      "\n",
      "üéØ Rate limiting test completed!\n"
     ]
    }
   ],
   "source": [
    "# Test rate limiting with dummy model (no real API calls)\n",
    "import time\n",
    "\n",
    "def test_rate_limiting():\n",
    "    \"\"\"Test the rate limiting functionality without making real API calls\"\"\"\n",
    "    \n",
    "    # Test with a model that has rate limits\n",
    "    test_model = \"gemini-2.0-flash\"\n",
    "    test_messages = [{\"role\": \"user\", \"content\": \"Test message\"}]\n",
    "    \n",
    "    print(f\"üß™ Testing rate limiting for model: {test_model}\")\n",
    "    print(f\"üìä Rate limits: {MODEL_LIMITS.get(test_model)}\")\n",
    "    print()\n",
    "    \n",
    "    # Simulate multiple rapid calls to test rate limiting\n",
    "    for i in range(5):\n",
    "        print(f\"Attempt {i+1}:\")\n",
    "        \n",
    "        # Check if model has rate limits\n",
    "        if test_model in MODEL_LIMITS:\n",
    "            limits = MODEL_LIMITS[test_model]\n",
    "            if limits.get(\"rpm\"):\n",
    "                # Create rate limiter\n",
    "                rpm_limiter = FixedWindowRateLimiter(storage)\n",
    "                rpm_limit = parse(f\"{limits['rpm']}/minute\")\n",
    "                \n",
    "                # Create unique key for this model\n",
    "                rate_limit_key = f\"api_call:{test_model}\"\n",
    "                \n",
    "                try:\n",
    "                    if rpm_limiter.hit(rpm_limit, rate_limit_key):\n",
    "                        print(f\"  ‚úÖ Rate limit check passed - would make API call\")\n",
    "                    else:\n",
    "                        print(f\"  ‚ö†Ô∏è Rate limit exceeded - API call blocked\")\n",
    "                        print(f\"  üìà Current limit: {limits['rpm']} requests per minute\")\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ùå Rate limiting error: {e}\")\n",
    "                    print(f\"  üîÑ Proceeding without rate limiting...\")\n",
    "            else:\n",
    "                print(f\"  ‚ÑπÔ∏è No RPM limit defined for this model\")\n",
    "        else:\n",
    "            print(f\"  ‚ÑπÔ∏è Model not found in MODEL_LIMITS - no rate limiting applied\")\n",
    "        \n",
    "        # Small delay to simulate real usage\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    print(\"\\nüéØ Rate limiting test completed!\")\n",
    "\n",
    "# Run the test\n",
    "test_rate_limiting()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive rate limiting test with different models\n",
    "def test_multiple_models():\n",
    "    \"\"\"Test rate limiting with different models to show various scenarios\"\"\"\n",
    "    \n",
    "    test_models = [\n",
    "        \"gemini-2.0-flash\",           # Has RPM limit (15)\n",
    "        \"gemini-2.5-flash-lite\",      # Has RPM limit (15) \n",
    "        \"gemini-2.5-flash-live\",      # No RPM limit (None)\n",
    "        \"unknown-model\",              # Not in MODEL_LIMITS\n",
    "    ]\n",
    "    \n",
    "    print(\"üî¨ Comprehensive Rate Limiting Test\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for model in test_models:\n",
    "        print(f\"\\nüìã Testing model: {model}\")\n",
    "        \n",
    "        if model in MODEL_LIMITS:\n",
    "            limits = MODEL_LIMITS[model]\n",
    "            print(f\"   üìä Limits: {limits}\")\n",
    "            \n",
    "            if limits.get(\"rpm\"):\n",
    "                print(f\"   üö¶ RPM Limit: {limits['rpm']} requests per minute\")\n",
    "                \n",
    "                # Test rate limiting\n",
    "                rpm_limiter = FixedWindowRateLimiter(storage)\n",
    "                rpm_limit = parse(f\"{limits['rpm']}/minute\")\n",
    "                rate_limit_key = f\"api_call:{model}\"\n",
    "                \n",
    "                # Try 3 rapid calls\n",
    "                for i in range(3):\n",
    "                    try:\n",
    "                        if rpm_limiter.hit(rpm_limit, rate_limit_key):\n",
    "                            print(f\"   ‚úÖ Call {i+1}: Allowed\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ö†Ô∏è Call {i+1}: Blocked (rate limit exceeded)\")\n",
    "                            break\n",
    "                    except Exception as e:\n",
    "                        print(f\"   ‚ùå Call {i+1}: Error - {e}\")\n",
    "            else:\n",
    "                print(f\"   ‚ÑπÔ∏è No RPM limit - unlimited requests\")\n",
    "        else:\n",
    "            print(f\"   ‚ùì Model not in MODEL_LIMITS - no rate limiting\")\n",
    "    \n",
    "    print(f\"\\nüéØ All tests completed!\")\n",
    "\n",
    "# Run comprehensive test\n",
    "test_multiple_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock version of callLLmApi for testing without real API calls\n",
    "def mock_callLLmApi(api_key, base_url, model, messages):\n",
    "    \"\"\"Mock version that simulates the rate limiting without real API calls\"\"\"\n",
    "    \n",
    "    print(f\"üîß Mock API call for model: {model}\")\n",
    "    print(f\"üìù Messages: {messages}\")\n",
    "    \n",
    "    # Initialize rate limiter for this model (same logic as real function)\n",
    "    if model in MODEL_LIMITS:\n",
    "        limits = MODEL_LIMITS[model]\n",
    "        print(f\"üìä Found limits: {limits}\")\n",
    "        \n",
    "        # Create rate limiter for requests per minute (rpm)\n",
    "        if limits.get(\"rpm\"):\n",
    "            rpm_limiter = FixedWindowRateLimiter(storage)\n",
    "            rpm_limit = parse(f\"{limits['rpm']}/minute\")\n",
    "            print(f\"üö¶ RPM limit: {limits['rpm']} requests per minute\")\n",
    "        else:\n",
    "            rpm_limiter = None\n",
    "            rpm_limit = None\n",
    "            print(f\"‚ÑπÔ∏è No RPM limit defined\")\n",
    "    else:\n",
    "        rpm_limiter = None\n",
    "        rpm_limit = None\n",
    "        print(f\"‚ùì Model not in MODEL_LIMITS - no rate limiting\")\n",
    "    \n",
    "    # Check rate limit before making API call (same logic as real function)\n",
    "    if rpm_limiter and rpm_limit:\n",
    "        rate_limit_key = f\"api_call:{model}\"\n",
    "        try:\n",
    "            if not rpm_limiter.hit(rpm_limit, rate_limit_key):\n",
    "                print(f\"‚ö†Ô∏è Rate limit exceeded for model {model}. Current limit: {limits['rpm']} requests per minute.\")\n",
    "                print(\"Please wait a moment before trying again.\")\n",
    "                return None\n",
    "            else:\n",
    "                print(f\"‚úÖ Rate limit check passed\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Rate limiting error: {e}\")\n",
    "            print(\"Proceeding without rate limiting...\")\n",
    "    \n",
    "    # Mock API response (instead of real API call)\n",
    "    print(f\"üåê [MOCK] Making API call to {base_url} with model {model}\")\n",
    "    mock_response = f\"Mock response from {model}: 'This is a simulated response to your query.'\"\n",
    "    \n",
    "    print(f\"üì§ Mock response: {mock_response}\")\n",
    "    return mock_response\n",
    "\n",
    "# Test the mock function\n",
    "print(\"üß™ Testing Mock callLLmApi Function\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "test_cases = [\n",
    "    (\"gemini-2.0-flash\", \"https://generativelanguage.googleapis.com/v1beta/openai/\"),\n",
    "    (\"gemini-2.5-flash-live\", \"https://generativelanguage.googleapis.com/v1beta/openai/\"),\n",
    "    (\"unknown-model\", \"https://api.example.com/v1/\"),\n",
    "]\n",
    "\n",
    "for model, base_url in test_cases:\n",
    "    print(f\"\\n--- Testing {model} ---\")\n",
    "    result = mock_callLLmApi(\"fake_key\", base_url, model, [{\"role\": \"user\", \"content\": \"Test message\"}])\n",
    "    print(f\"Result: {result}\")\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = []\n",
    "answers = []\n",
    "messages = [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The API we know well\n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anthropic has a slightly different API, and Max Tokens is required\n",
    "\n",
    "model_name = \"claude-3-7-sonnet-latest\"\n",
    "\n",
    "claude = Anthropic()\n",
    "response = claude.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "answer = response.content[0].text\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "\n",
    "response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "model_name = \"deepseek-chat\"\n",
    "\n",
    "response = deepseek.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "model_name = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "response = groq.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the next cell, we will use Ollama\n",
    "\n",
    "Ollama runs a local web service that gives an OpenAI compatible endpoint,  \n",
    "and runs models locally using high performance C++ code.\n",
    "\n",
    "If you don't have Ollama, install it here by visiting https://ollama.com then pressing Download and following the instructions.\n",
    "\n",
    "After it's installed, you should be able to visit here: http://localhost:11434 and see the message \"Ollama is running\"\n",
    "\n",
    "You might need to restart Cursor (and maybe reboot). Then open a Terminal (control+\\`) and run `ollama serve`\n",
    "\n",
    "Useful Ollama commands (run these in the terminal, or with an exclamation mark in this notebook):\n",
    "\n",
    "`ollama pull <model_name>` downloads a model locally  \n",
    "`ollama ls` lists all the models you've downloaded  \n",
    "`ollama rm <model_name>` deletes the specified model from your downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Super important - ignore me at your peril!</h2>\n",
    "            <span style=\"color:#ff7800;\">The model called <b>llama3.3</b> is FAR too large for home computers - it's not intended for personal computing and will consume all your resources! Stick with the nicely sized <b>llama3.2</b> or <b>llama3.2:1b</b> and if you want larger, try llama3.1 or smaller variants of Qwen, Gemma, Phi or DeepSeek. See the <A href=\"https://ollama.com/models\">the Ollama models page</a> for a full list of models and sizes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "model_name = \"llama3.2\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So where are we?\n",
    "\n",
    "print(competitors)\n",
    "print(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's nice to know how to use \"zip\"\n",
    "for competitor, answer in zip(competitors, answers):\n",
    "    print(f\"Competitor: {competitor}\\n\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's bring this together - note the use of \"enumerate\"\n",
    "\n",
    "together = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"# Response from competitor {index+1}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"You are judging a competition between {len(competitors)} competitors.\n",
    "Each model has been given this question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
    "Respond with JSON, and only JSON, with the following format:\n",
    "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
    "\n",
    "Here are the responses from each competitor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_messages = [{\"role\": \"user\", \"content\": judge}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judgement time!\n",
    "\n",
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    messages=judge_messages,\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK let's turn this into results!\n",
    "\n",
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"results\"]\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor = competitors[int(result)-1]\n",
    "    print(f\"Rank {index+1}: {competitor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Exercise</h2>\n",
    "            <span style=\"color:#ff7800;\">Which pattern(s) did this use? Try updating this to add another Agentic design pattern.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Commercial implications</h2>\n",
    "            <span style=\"color:#00bfff;\">These kinds of patterns - to send a task to multiple models, and evaluate results,\n",
    "            are common where you need to improve the quality of your LLM response. This approach can be universally applied\n",
    "            to business projects where accuracy is critical.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
